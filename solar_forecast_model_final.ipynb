{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b6366e-a5a1-476e-8262-ada11e3d16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import numpy as np  # For handling numbers and arrays (tables of data)\n",
    "from math import sqrt  # To calculate square roots\n",
    "from sklearn.metrics import mean_squared_error  # To measure how far off our predictions are\n",
    "from matplotlib import pyplot as plt  # For drawing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d2402-492e-4893-9e38-3c2d3092b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the machine learning tools from Keras\n",
    "from keras.models import Sequential  # Helps us build a model one layer at a time\n",
    "from keras.layers import Dense, LSTM  # The types of layers we'll use in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f21dd-ebac-486c-9d5a-b7d40b567205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the training, validation, and test data from CSV files\n",
    "# These files contain the weather forecast inputs and solar energy outputs\n",
    "# The original code used Windows paths. Here we use relative paths for compatibility with Jupyter\n",
    "train_data = np.loadtxt(\"train_NREL_solar_data.csv\", delimiter=',')\n",
    "validate_data = np.loadtxt(\"validate_NREL_solar_data.csv\", delimiter=',')\n",
    "test_data = np.loadtxt(\"test_NREL_solar_data.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162776e-85bf-454a-92e0-e549f0013983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the data into inputs and outputs\n",
    "# Inputs = weather conditions (first 9 columns), Output = solar irradiance (last column)\n",
    "x_tr, t_tr = train_data[:, 0:9], train_data[:, -1]  # Training data\n",
    "x_va, t_va = validate_data[:, 0:9], validate_data[:, -1]  # Validation data\n",
    "x_te, t_te = test_data[:, 0:9], test_data[:, -1]  # Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6162ad-a954-488e-8e8f-a3e1f1ab40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate the number of full days in each dataset\n",
    "# Each \"day\" has 11 time steps (e.g., 11 hourly records)\n",
    "Ndays_tr = x_tr.shape[0] // 11\n",
    "Ndays_va = x_va.shape[0] // 11\n",
    "Ndays_te = x_te.shape[0] // 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9385cbd-50b0-42f7-a82b-1e2fd98eb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Reshape the data so the model can understand it\n",
    "# LSTM models expect input in the shape: [samples, time steps, features]\n",
    "train_x = x_tr.reshape(Ndays_tr, 11, 9)\n",
    "train_t = t_tr.reshape(Ndays_tr, 11, 1)\n",
    "\n",
    "validate_x = x_va.reshape(Ndays_va, 11, 9)\n",
    "validate_t = t_va.reshape(Ndays_va, 11, 1)\n",
    "\n",
    "test_x = x_te.reshape(Ndays_te, 11, 9)\n",
    "test_t = t_te.reshape(Ndays_te, 11, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142db137-be61-4dfe-ab00-8e2c50d5b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build the LSTM model\n",
    "model = Sequential()  # A basic model where we add one layer at a time\n",
    "\n",
    "# Add an LSTM layer with 50 units\n",
    "# This layer learns patterns in sequences over time\n",
    "model.add(LSTM(50, input_shape=(11, 9), return_sequences=True))\n",
    "\n",
    "# Add a Dense output layer that gives one prediction per time step\n",
    "# Linear activation means it outputs a real number (not a category)\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model using Mean Squared Error (MSE) as the loss function\n",
    "# The model will try to reduce this error during training\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc75ab-cbbb-42de-ab29-2e55a056cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train the model\n",
    "# The model looks at the training data 100 times (100 epochs)\n",
    "# It also checks how it performs on the validation data during training\n",
    "history = model.fit(train_x, train_t, epochs=100, batch_size=50, validation_data=(validate_x, validate_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d04487-e3a5-4b10-b72a-53d1621fc357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Plot the training loss to see how well the model learned\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (Mean Squared Error)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa6998-f302-4f8e-8251-3ccc506748e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Use the trained model to predict solar irradiance on the test data\n",
    "yhat = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af71e59-b03d-470d-87bc-6c16f89cd753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Reshape the predictions into a flat list for comparison\n",
    "y_te = yhat.reshape(Ndays_te * 11,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df6544-e181-42d7-8cb5-e7fb4ec6322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Calculate RMSE (Root Mean Squared Error) to measure accuracy\n",
    "# The lower the RMSE, the better the model's performance\n",
    "rmse2 = mean_squared_error(y_te, t_te) * Ndays_te * 11\n",
    "rmse = sqrt(rmse2 / 4026) * 1087.4396 / 2  # Scaling as used in the original study\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e03227-d738-4d69-bbeb-37a365cddbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 11 - Load the CSV files and check for missing values using NumPy\n",
    "\n",
    "import numpy as np  # Make sure this is already imported earlier\n",
    "\n",
    "# Load the CSVs (assuming no headers and same structure)\n",
    "train_data = np.loadtxt('train_NREL_solar_data.csv', delimiter=',')\n",
    "validate_data = np.loadtxt('validate_NREL_solar_data.csv', delimiter=',')\n",
    "test_data = np.loadtxt('test_NREL_solar_data.csv', delimiter=',')\n",
    "\n",
    "# Preview the training data\n",
    "print(\"Preview of training data (first 5 rows):\")\n",
    "print(train_data[:5])\n",
    "\n",
    "# Check for missing values in training data\n",
    "print(\"\\nMissing values in training data (per column):\")\n",
    "print(np.isnan(train_data).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e0e50-942f-49b0-8f34-f3ccf2415841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# STEP 12 - Reshape the data into sequences for CNN-LSTM\n",
    "def create_sequences(data, time_steps):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i+time_steps, :])     # Input window\n",
    "        y.append(data[i+time_steps, -1])      # Target value\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Set how many time steps to look back\n",
    "time_steps = 10\n",
    "\n",
    "# Apply to the training dataset\n",
    "X_train, y_train = create_sequences(train_data, time_steps)\n",
    "\n",
    "# Check the shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf64b0-6a4a-4e42-beab-7958d18e207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense\n",
    "\n",
    "# STEP 12.1 - Define the CNN-LSTM model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# STEP 12.2 - Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# STEP 12.3 - Train the model (use fewer epochs if you're testing)\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6b57a-e811-4f25-abe8-33089a691c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# STEP 13 - Plot the training loss over epochs\n",
    "plt.plot(history.history['loss'], marker='o')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde6571-7fb9-4469-8bd7-b4242ded1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 14: Load and scale the data using NumPy only\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load CSVs using NumPy (assuming no headers)\n",
    "train_data = np.loadtxt('train_NREL_solar_data.csv', delimiter=',')\n",
    "validate_data = np.loadtxt('validate_NREL_solar_data.csv', delimiter=',')\n",
    "test_data = np.loadtxt('test_NREL_solar_data.csv', delimiter=',')\n",
    "\n",
    "# Scale all datasets using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "validate_data = scaler.transform(validate_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "# Preview the first 5 rows of the scaled training data\n",
    "print(\"Training data after scaling (first 5 rows):\")\n",
    "print(train_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197dc126-2448-4d10-a2ec-5b4c884f83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 15: Reshape into sequences\n",
    "import numpy as np\n",
    "\n",
    "def create_sequences(data, time_steps):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i+time_steps])\n",
    "        y.append(data[i+time_steps, -1])  # # Save the trained CNN-LSTM model to disk for future reuse\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Set time step size\n",
    "time_steps = 10\n",
    "\n",
    "# Create sequences\n",
    "X_train, y_train = create_sequences(train_data, time_steps)\n",
    "X_val, y_val = create_sequences(validate_data, time_steps)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd934d78-7061-4dad-998c-f5af14ad01a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 16: Build CNN-LSTM Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100, batch_size=32,\n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a5494-bfba-43e1-ba70-da0d5b2cb5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# STEP 17: Visualise Model Learning Over Time\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', marker='x')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6dea30-9fbb-462a-9188-4bc6f7ece2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "# Step 18: Plot first 100 predictions vs actual values\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(predictions[:100], label='Predicted', linestyle='--')\n",
    "plt.plot(y_val[:100], label='Actual', alpha=0.7)\n",
    "plt.title('Predicted vs Actual Solar Irradiance on Validation Set (First 100 Samples)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Scaled Irradiance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be47cff3-7970-4eb2-94e5-0aa4a31ef130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 19: Save the trained CNN-LSTM model to disk for future reuse\n",
    "model.save('cnn_lstm_solar_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10e121-2fdc-4f4d-8aa2-644d5c48d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 20: Reuse your sequence function and prepare test set sequences using NumPy only\n",
    "\n",
    "# Sequence creation function using NumPy slicing\n",
    "def create_sequences(data, time_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i+time_steps, :])       # All features in the window\n",
    "        y.append(data[i+time_steps, -1])        # Target = irradiance column at next step\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load and scale the test data using NumPy\n",
    "test_data_raw = np.loadtxt('test_NREL_solar_data.csv', delimiter=',')\n",
    "scaler = MinMaxScaler()\n",
    "test_data = scaler.fit_transform(test_data_raw)  # Fit just for testing\n",
    "\n",
    "# Prepare test sequences for model input\n",
    "time_steps = 10\n",
    "X_test, y_test = create_sequences(test_data, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed606e70-29f7-4bc9-9bd6-f4179e1759f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 21: Load the trained model (if not in memory)\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('cnn_lstm_solar_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6cf4c6-bac9-4a54-8be2-c08c5b0f6794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 22: Predict on test set\n",
    "predictions_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9dfda1-71b7-456f-b40b-ef3f473dbdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 23: Evaluate model using RMSE\n",
    "rmse_test = sqrt(mean_squared_error(y_test, predictions_test))\n",
    "print(f\"Test RMSE: {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf61ac9-5135-4771-8cdc-159d97d05e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 24: Calculate and print RMSE on test set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, predictions_test))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6aa3b-86a4-4c13-8484-dd12adbc3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 25: Plot predictions vs actual values\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(predictions_test[:100], label='Predicted', linestyle='--')\n",
    "plt.plot(y_test[:1â€º00], label='Actual', alpha=0.7)\n",
    "plt.title('Predicted vs Actual Solar Irradiance on Test Set (First 100 Samples)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Scaled Irradiance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4b326-02f8-4fc3-a034-746837de14b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
